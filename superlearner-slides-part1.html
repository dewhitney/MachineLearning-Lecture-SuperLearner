<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Super Learner (Part 1)</title>
    <meta charset="utf-8" />
    <meta name="author" content="David Whitney" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css/hygge.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">


background-image: url(https://github.com/dewhitney/MachineLearning-Lecture-SuperLearner/blob/main/supermanlearner.png?raw=true)
background-size: contain
background-position: right
class: left, middle



.topnote[![:scale 25%](lshtm-logo.png)]

# Super Learner

## David Whitney* 

### Machine Learning 2022

.footnote[*based on Prof Karla Diaz-Ordaz's 2021 materials]

---
class: left

# Learning Outcomes
.content-box-blue[
After completing today's session, you will
- understand the basic idea of Super Learner (SL)

- understand the 3 important choices to be made for using SL

- be able to apply SL in a practical example

- know how to use cross-validation to measure the performance of a SL]

---
class: left

# Motivation

- You have experience with a wide library of prediction models (learners):
  
&lt;img src="superlearner-slides-part1_files/figure-html/unnamed-chunk-1-1.jpeg" style="display: block; margin: auto;" /&gt;
  
- Your collaborators/colleagues may have many more!

- For a given application, there may be several candidate learners

- How might you choose between the candidates?

---
class: left

# Super Learner: a bird's eye view

The **Super Learner** is a meta-learning algorithm that
.content-box-blue[
- Takes the data and a library `\(\mathcal{L}\)` of `\(L\)` candidate learners as inputs

- Estimates performance of each learner using `\(V\)`-fold cross-validation

- Creates an optimal ensemble `\(m(Z;\boldsymbol{\alpha})\)` of the learners ]

--

- The ensemble `\(m(Z;\boldsymbol{\alpha})\)` offers a reproducible way to choose from (or combine) predictions from the candidate learners

--

- Super Learner ensembles also possess desirable (large sample) guarantees
  
---
class: left

# Super Learner: a bird's eye .blue[**view**]

![:scale 105%](SL-schematic.svg)

---
class: left
# What makes a Super Learner?

Before we can follow the steps from above, we must specify
.content-box-blue[
1. The library `\(\mathcal{L}\)` of candidate learners

2. A loss function `\(\ell\)` to measure performance

3. Form of the meta-learner `\(m(Z;\alpha)\)`
  ]
---
class: left
# Library

- Common algorithms to consider
.content-box-blue[
.pull-left[
- Linear regression
- Polynomial linear regression
- Random forest
- Bagging
]
.pull-right[
- GAMs
- Gradient boosting
- Neural network
- Polynomial spline regression
]]


- Select the learner algorithms using contextual knowledge of the problem

- If a custom-made algorithm (eg a clinical prediction model) is known to perform well, include it!

- It is recommended to include a parametric model

---
class: left
# Loss function

- The loss function is often the squared error loss `\(\ell = (Y - f(X))^2\)`

&lt;img src="superlearner-slides-part1_files/figure-html/unnamed-chunk-2-1.jpeg" style="display: block; margin: auto;" /&gt;

--

.content-box-blue[
- For binary outcomes where `\(Y=0,1\)`:

  - Negative binomial log likelihood 
  
  - Rank loss function: maximises the area under the ROC curve (a function of both sensitivity and specificity), thus optimizing the algorithms ability to correctly classify observations.]

---
class: left

# Meta-regression model

- We often use non-negative least squares (NNLS)

$$ E[Y|X=x] = \alpha_1 \hat{f}_1(x) + \ldots + \alpha_L \hat{f}_L(x) $$
- In NNLS, `\(\alpha_l \geq 0\)` and `\(\sum_{l=1}^L \alpha_l = 1\)`

- Thus `\(m(Z_i; \alpha)\)` is a convex combination of the learners in `\(\mathcal{L}\)` with weights `\(\alpha\)` chosen to minimise the CV risk of SL:

$$ \hat{\alpha} = \arg \min_\alpha \sum_i (Y_i - m(Z_i; \alpha))^2 $$

- We obtain a **discrete super learner** by instead placing all the weight on the learner `\(\hat{f}_l\)` that performs best using `\(V\)`-fold CV

---
class:center,inverse,middle

# Super Learner in R

---
class: left

# Super Learner: a bird's eye .blue[**view**]

![:scale 105%](SL-schematic.svg)

---
class: left

# Super Learner in R

- You will need the .red[`SuperLearner`] package installed for the practical:


```r
SL_installed &lt;- require(SuperLearner)

if(!SL_installed) install.packages("SuperLearner")

library(SuperLearner)
```

---

class: left
# The SuperLearner function

- The main function in .red[`SuperLearner`] is itself called .red[`SuperLearner`]:


```r
args(SuperLearner)
```

```
## function (Y, X, newX = NULL, family = gaussian(), SL.library, 
##     method = "method.NNLS", id = NULL, verbose = FALSE, control = list(), 
##     cvControl = list(), obsWeights = NULL, env = parent.frame()) 
## NULL
```

.content-box-blue[
Key arguments for customising your Super Learner:
- .red[`SL.library`]: character vector of learners (this is `\(\mathcal{L}\)`)

- .red[`method`]: method to estimate meta-learner (based on `\(\ell\)` and `\(m(Z|\alpha)\)`)

- Enter .red[`?SuperLearner`] in your console for more details
]

---
class: left
# Note: when coding learners for your `\(\mathcal{L}\)`...

- .red[`SuperLearner`] uses a standard interface to automate training/prediction

- This is achieved via wrapper functions that follow this template:


```r
SuperLearner::SL.template
```

```
## function (Y, X, newX, family, obsWeights, id, ...) 
## {
##     if (family$family == "gaussian") {
##     }
##     if (family$family == "binomial") {
##     }
##     pred &lt;- numeric()
##     fit &lt;- vector("list", length = 0)
##     class(fit) &lt;- c("SL.template")
##     out &lt;- list(pred = pred, fit = fit)
##     return(out)
## }
## &lt;bytecode: 0x7fcd883cec38&gt;
## &lt;environment: namespace:SuperLearner&gt;
```

---
class: left
# Built-in wrappers

- You **might** be able to use a built-in wrapper for your learner


```r
listWrappers("SL")
```

```
## All prediction algorithm wrappers in SuperLearner:
```

```
##  [1] "SL.bartMachine"      "SL.bayesglm"         "SL.biglasso"        
##  [4] "SL.caret"            "SL.caret.rpart"      "SL.cforest"         
##  [7] "SL.earth"            "SL.extraTrees"       "SL.gam"             
## [10] "SL.gbm"              "SL.glm"              "SL.glm.interaction" 
## [13] "SL.glmnet"           "SL.ipredbagg"        "SL.kernelKnn"       
## [16] "SL.knn"              "SL.ksvm"             "SL.lda"             
## [19] "SL.leekasso"         "SL.lm"               "SL.loess"           
## [22] "SL.logreg"           "SL.mean"             "SL.nnet"            
## [25] "SL.nnls"             "SL.polymars"         "SL.qda"             
## [28] "SL.randomForest"     "SL.ranger"           "SL.ridge"           
## [31] "SL.rpart"            "SL.rpartPrune"       "SL.speedglm"        
## [34] "SL.speedlm"          "SL.step"             "SL.step.forward"    
## [37] "SL.step.interaction" "SL.stepAIC"          "SL.svm"             
## [40] "SL.template"         "SL.xgboost"
```

---
class: left

# Example: a univariate Super Learner

.pull-left[
Generating some data:


```r
set.seed(2490) #reproducible
n &lt;- 50
x &lt;- runif(n, min=-4, max=4)
f &lt;- function(x) 
     {
       .4*x-.4*x^2+.05*x^3
     }
y &lt;- f(x) + rnorm(n)
```
]

.pull-right[
The data and (unknown) regression:

&lt;img src="superlearner-slides-part1_files/figure-html/unnamed-chunk-8-1.png" width="100%" /&gt;
]

---

- Let's use GLM and random forest with the NNLS meta-learner


```r
my.lib &lt;- c("SL.randomForest", "SL.glm")

fit &lt;- SuperLearner(Y = y, X = data.frame(x), 
                    newX = data.frame(x=x0), 
                    SL.library = my.lib, 
                    method = "method.NNLS",
                    cvControl = list(V=5L),
                    control = list(saveCVFitLibrary=TRUE))

SL.pred &lt;- fit$SL.predict
```

&lt;img src="superlearner-slides-part1_files/figure-html/unnamed-chunk-10-1.png" style="display: block; margin: auto;" /&gt;

---
# Step 0. fit each learner in `\(\mathcal{L}\)` on all data
The `\(L \times n\)` matrix of full-data predictions is stored as .red[`fit$library.predict`]

![](superlearner-slides-part1_files/figure-html/unnamed-chunk-11-1.jpeg)&lt;!-- --&gt;

---
# Step 1. Split the data into `\(V=5\)` blocks
The row numbers for the `\(V\)`-fold cross-validation are stored as a list


```r
fit$validRows #think "valid-ation sets 1-5"
```

```
## $`1`
##  [1] 31 35 44 20 22 13 42 25 11 18
## 
## $`2`
##  [1] 39 28 40 15 24  4 45 49 37  6
## 
## $`3`
##  [1] 21 26 47  7 36 43  9 27 14 29
## 
## $`4`
##  [1] 19 30 48  1 50  3 34 12 10 32
## 
## $`5`
##  [1]  8 46 17  2  5 38 33 23 41 16
```

---
# Steps 2-3. Train each learner, then predict
Recall that we do this `\(L \times V\)` times (train on `\(V-1\)` folds, predict on last fold)

&lt;img src="superlearner-slides-part1_files/figure-html/unnamed-chunk-13-1.png" style="display: block; margin: auto;" /&gt;

.footnote[(We plot training sets as solid blue, test sets as empty red)]

---
# Step 3. Fit the meta-learner

- The `\(n\times L\)` matrix of fitted values on the training data are stored in .red[`fit$Z`]
  - Remember each data point is belongs to **exactly one** validation set!


```r
head(fit$Z)
```

```
##             [,1]       [,2]
## [1,] -0.31717312 -2.2417058
## [2,] -0.80226697  0.6553930
## [3,] -1.24058623  0.4831672
## [4,] -1.86230006  1.5361682
## [5,] -0.03448410 -0.1135900
## [6,] -0.06711427 -0.9365990
```

- The coefficient vector from NNLS is available as .red[`fit$coef`]


```r
fit$coef
```

```
## SL.randomForest_All          SL.glm_All 
##         0.994654219         0.005345781
```

- Here, most of the weight is placed on random forest

---
# Step 4. Predict on new data

- Predictions for .red[`newX`] are at .red[`fit$SL.predict`]

&lt;img src="superlearner-slides-part1_files/figure-html/unnamed-chunk-16-1.png" style="display: block; margin: auto;" /&gt;

---
class: inverse, center, middle

# Your turn to practice!

---



    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"ratio": "4:3",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
